{"paragraphs":[{"title":"Perguntas sobre Spark - Teste Semantix","text":"%md\n### Qual o objetivo do comando cache em Spark?\nPersiste(grava) dados em memória, é uma técnica de otimização de performance, dados parciais podem ser persistidos em memória para acelerar computações ou reutizados nos processamentos para evitar repetições(actions, ações) desnecessárias e assim aumentar a performance de um longo processamento.\n\n### O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\nO modelo de aplicativo do Spark através do uso de RDDs(coleções distribuídas), \"lazy evaluation\", e DAG(Graphs de dependência entre as operações) permite processamento em paralelo(MPP) muito mais simplificado e eficiente que o modelo map/reduce Java no Hadoop, um aplicativo desenvolvido em Spark(Scala ou Python) também é muito mais simples se comparado a mesma implementação em Java map/reduce.\n\n### Qual é a função do SparkContext ?\nÉ a criação de uma instância de aplicativo no Spark, é uma analogia a uma conexão a um banco de dados relacional onde você especifica os parâmetros de configuração, memória, e recursos, nele você cria a conexão e o aplicativo, depois é utilizado para criar sessions(SparkSessions, SQLContext, HIVEContext), e outras instâncias do Spark Core\n\n### Explique com suas palavras o que é Resilient Distributed Datasets (RDD).\nÉ a abstração de dados básica do Spark, imutável, particionado e distribuído, esta abstração é a base da computação paralela do Spark.\n\n### GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?\nGroupByKey e reduceByKey são \"Wide transformations\", ou seja, produzem multiplas referências de dependências das partições filhas, mas na transformação reduceByKey o Spark combina a saída em uma chave comum em cada partição antes de fazer \"shuffle\" nos dados, o mesmo não acontece com GroupBykey onde todos os pares chave-valor fazem \"shuffle\" por entre os executores, causando muito mais processamento para o agrupamento, e uma função .sum deve ser aplicada e só é executada após todo o agrupamento ser concluído.\n\n\n### Explique o que o código Scala abaixo faz.\n\n    val textFile = sc.textFile ( \"hdfs://...\" )\n    val counts = textFile . flatMap ( line => line . split ( \" \" ))\n        .map ( word => ( word , 1 ))\n        .reduceByKey ( _ + _ )\n    counts.saveAsTextFile ( \"hdfs://...\" )\n\n***Geral***: Este código é uma implementação simples de Word Count(contar palavras em um texto).\n***Passo a passo***:\n1 - Lê um ou mais arquivos de texto do armazenamento HDFS do Hadoop.\n2 - Serializa o texto com a função flatMap, lê uma linha por vez.\n3 - Para cada linha faz as transformações ->separa as palavras por espaço-> para cada palavra faz um map e cria uma tupla com o número 1->para cada tupla agrupa pela chave palavra e soma o número(_ + _)->atribui a saída a counts.\n4 - Grava o conteúdo de counts em um arquivo de texto em um armazenamento HDFS no Hadoop.\n\n\n**Autor**: Ricardo Kurz dos Santos","user":"admin","dateUpdated":"2018-06-04T11:10:00-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Qual o objetivo do comando cache em Spark?</h3>\n<p>Persiste(grava) dados em memória, é uma técnica de otimização de performance, dados parciais podem ser persistidos em memória para acelerar computações ou reutizados nos processamentos para evitar repetições(actions, ações) desnecessárias e assim aumentar a performance de um longo processamento.</p>\n<h3>O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?</h3>\n<p>O modelo de aplicativo do Spark através do uso de RDDs(coleções distribuídas), &ldquo;lazy evaluation&rdquo;, e DAG(Graphs de dependência entre as operações) permite processamento em paralelo(MPP) muito mais simplificado e eficiente que o modelo map/reduce Java no Hadoop, um aplicativo desenvolvido em Spark(Scala ou Python) também é muito mais simples se comparado a mesma implementação em Java map/reduce.</p>\n<h3>Qual é a função do SparkContext ?</h3>\n<p>É a criação de uma instância de aplicativo no Spark, é uma analogia a uma conexão a um banco de dados relacional onde você especifica os parâmetros de configuração, memória, e recursos, nele você cria a conexão e o aplicativo, depois é utilizado para criar sessions(SparkSessions, SQLContext, HIVEContext), e outras instâncias do Spark Core</p>\n<h3>Explique com suas palavras o que é Resilient Distributed Datasets (RDD).</h3>\n<p>É a abstração de dados básica do Spark, imutável, particionado e distribuído, esta abstração é a base da computação paralela do Spark.</p>\n<h3>GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?</h3>\n<p>GroupByKey e reduceByKey são &ldquo;Wide transformations&rdquo;, ou seja, produzem multiplas referências de dependências das partições filhas, mas na transformação reduceByKey o Spark combina a saída em uma chave comum em cada partição antes de fazer &ldquo;shuffle&rdquo; nos dados, o mesmo não acontece com GroupBykey onde todos os pares chave-valor fazem &ldquo;shuffle&rdquo; por entre os executores, causando muito mais processamento para o agrupamento, e uma função .sum deve ser aplicada e só é executada após todo o agrupamento ser concluído.</p>\n<h3>Explique o que o código Scala abaixo faz.</h3>\n<pre><code>val textFile = sc.textFile ( \"hdfs://...\" )\nval counts = textFile . flatMap ( line =&gt; line . split ( \" \" ))\n    .map ( word =&gt; ( word , 1 ))\n    .reduceByKey ( _ + _ )\ncounts.saveAsTextFile ( \"hdfs://...\" )\n</code></pre>\n<p><strong><em>Geral</em></strong>: Este código é uma implementação simples de Word Count(contar palavras em um texto).\n<br  /><strong><em>Passo a passo</em></strong>:\n<br  />1 - Lê um ou mais arquivos de texto do armazenamento HDFS do Hadoop.\n<br  />2 - Serializa o texto com a função flatMap, lê uma linha por vez.\n<br  />3 - Para cada linha faz as transformações ->separa as palavras por espaço-> para cada palavra faz um map e cria uma tupla com o número 1->para cada tupla agrupa pela chave palavra e soma o número(<em> + </em>)->atribui a saída a counts.\n<br  />4 - Grava o conteúdo de counts em um arquivo de texto em um armazenamento HDFS no Hadoop.</p>\n<p><strong>Autor</strong>: Ricardo Kurz dos Santos</p>\n"}]},"apps":[],"jobName":"paragraph_1528116501102_1859073309","id":"20180602-145747_930816613","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:10:00-0300","dateFinished":"2018-06-04T11:10:00-0300","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6960"},{"title":"Efetua o download dos arquivos de log da Nasa","text":"%sh\nrm -f NASA_access_log*\nwget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz -q\nwget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz -q\n\ngunzip NASA_access_log_Jul95.gz\ngunzip NASA_access_log_Aug95.gz\nls","user":"admin","dateUpdated":"2018-06-04T11:10:00-0300","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"NASA_access_log_Aug95\nNASA_access_log_Jul95\n"}]},"apps":[],"jobName":"paragraph_1528116501110_1868307283","id":"20180601-205125_789046592","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:10:00-0300","dateFinished":"2018-06-04T11:12:08-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6961"},{"title":"Carga dos arquivos no storage HDFS","text":"%sh\nhdfs dfs -rm -f /tmp/NASA_access_log*\nhdfs dfs -put NASA_access_log* /tmp/\nhdfs dfs -ls /tmp/NASA_access_log*","user":"admin","dateUpdated":"2018-06-04T11:12:30-0300","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"18/06/04 11:12:33 INFO fs.TrashPolicyDefault: Moved: 'hdfs://master01.blackdragon.local:8020/tmp/NASA_access_log_Jul95.gz' to trash at: hdfs://master01.blackdragon.local:8020/user/zeppelin/.Trash/Current/tmp/NASA_access_log_Jul95.gz\n-rw-r--r--   3 zeppelin hdfs  167813770 2018-06-04 11:12 /tmp/NASA_access_log_Aug95\n-rw-r--r--   3 zeppelin hdfs  205242368 2018-06-04 11:12 /tmp/NASA_access_log_Jul95\n"}]},"apps":[],"jobName":"paragraph_1528116501112_1865998789","id":"20180601-205136_1385488547","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:12:30-0300","dateFinished":"2018-06-04T11:12:49-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6962"},{"title":"Lê os arquivos no HDFS e grava em um dataframe","text":"%spark2\nval log_file = spark.read.text(\"/tmp/NASA_access_log*\")","user":"admin","dateUpdated":"2018-06-04T11:12:59-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"log_file: org.apache.spark.sql.DataFrame = [value: string]\n"}]},"apps":[],"jobName":"paragraph_1528116501113_1865614041","id":"20180601-205859_483777166","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:13:00-0300","dateFinished":"2018-06-04T11:13:00-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6963"},{"title":"Pré-visualização dos textos de log","text":"%spark2\nlog_file.show(5, truncate=false)","user":"admin","dateUpdated":"2018-06-04T11:13:05-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                     |\n+--------------------------------------------------------------------------------------------------------------------------+\n|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839|\n|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0                                                   |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0                          |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0                        |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0                           |\n+--------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1528116501114_1866768287","id":"20180601-212638_774838922","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:13:05-0300","dateFinished":"2018-06-04T11:13:06-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6964"},{"title":"Extração dos campos usando Regex, cria um novo dataframe com as colunas correspondentes às informações, faz cache em memória para acesso rápido","text":"%spark2\nimport org.apache.spark.sql.functions._\n\nval log_file_df = log_file.select(regexp_extract($\"value\", \"^([^\\\\s]+\\\\s)\", 1).as(\"host\"),\n    regexp_extract($\"value\", \"^.*\\\\[(\\\\d\\\\d/\\\\w{3}/\\\\d{4}:\\\\d{2}:\\\\d{2}:\\\\d{2} -\\\\d{4})]\", 1).as(\"timestamp\"),\n    regexp_extract($\"value\", \"^.*\\\"\\\\w+\\\\s+([^\\\\s]+)\\\\s+HTTP.*\\\"\", 1).as(\"requisicao\"),\n    regexp_extract($\"value\", \"^.*\\\"\\\\s+([^\\\\s]+)\", 1).cast(\"integer\").as(\"codigo_http\"),\n    regexp_extract($\"value\", \"^.*\\\\s+(\\\\d+)$\", 1).cast(\"integer\").as(\"bytes_retorno\")\n    )\nlog_file_df.cache()\n","user":"admin","dateUpdated":"2018-06-04T11:13:12-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nlog_file_df: org.apache.spark.sql.DataFrame = [host: string, timestamp: string ... 3 more fields]\nres35: log_file_df.type = [host: string, timestamp: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1528116501115_1866383538","id":"20180601-212747_908970865","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:13:12-0300","dateFinished":"2018-06-04T11:13:13-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6965"},{"title":"Pré-visualização e validação do esquema","text":"%spark2\nlog_file_df.columns\nlog_file_df.printSchema","user":"admin","dateUpdated":"2018-06-04T11:13:25-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res36: Array[String] = Array(host, timestamp, requisicao, codigo_http, bytes_retorno)\nroot\n |-- host: string (nullable = true)\n |-- timestamp: string (nullable = true)\n |-- requisicao: string (nullable = true)\n |-- codigo_http: integer (nullable = true)\n |-- bytes_retorno: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1528116501115_1866383538","id":"20180601-213402_2021668006","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:13:25-0300","dateFinished":"2018-06-04T11:13:25-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6966"},{"title":"Validação das colunas numéricas quanto aos nulos","text":"%spark2\nlog_file.filter($\"value\".isNull).count()\nlog_file.filter(!$\"value\".rlike(\"\\\\d+$\")).show(5, truncate=false)\nlog_file_df.filter($\"codigo_http\".isNull).count()\nlog_file_df.filter($\"bytes_retorno\".isNull).count()\nlog_file_df.filter($\"bytes_retorno\".isNull).groupBy($\"codigo_http\").count().show()","user":"admin","dateUpdated":"2018-06-04T11:13:30-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res38: Long = 0\n+---------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                      |\n+---------------------------------------------------------------------------------------------------------------------------+\n|gw1.att.com - - [01/Aug/1995:00:03:53 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -                            |\n|js002.cc.utsunomiya-u.ac.jp - - [01/Aug/1995:00:07:33 -0400] \"GET /shuttle/resources/orbiters/discovery.gif HTTP/1.0\" 404 -|\n|tia1.eskimo.com - - [01/Aug/1995:00:28:41 -0400] \"GET /pub/winvn/release.txt HTTP/1.0\" 404 -                               |\n|itws.info.eng.niigata-u.ac.jp - - [01/Aug/1995:00:38:01 -0400] \"GET /ksc.html/facts/about_ksc.html HTTP/1.0\" 403 -         |\n|grimnet23.idirect.com - - [01/Aug/1995:00:50:12 -0400] \"GET /www/software/winvn/winvn.html HTTP/1.0\" 404 -                 |\n+---------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\nres40: Long = 1\nres41: Long = 33905\n+-----------+-----+\n|codigo_http|count|\n+-----------+-----+\n|        501|   41|\n|       null|    1|\n|        400|   15|\n|        403|  225|\n|        404|20900|\n|        200|  161|\n|        302|12562|\n+-----------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1528116501116_1864459794","id":"20180602-180131_379506070","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:13:30-0300","dateFinished":"2018-06-04T11:14:50-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6967"},{"title":"Existem 33905 ocorrências de nulo em \"bytes_retorno\", usando o método \"na.fill\" para preencher com zeros, atribui em um novo dataframe","text":"%spark2\nval log_file_df_clean = log_file_df.na.fill(0)\nlog_file_df_clean.filter($\"bytes_retorno\".isNull).groupBy($\"codigo_http\").count().show()","user":"admin","dateUpdated":"2018-06-04T11:19:14-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"log_file_df_clean: org.apache.spark.sql.DataFrame = [host: string, timestamp: string ... 3 more fields]\n+-----------+-----+\n|codigo_http|count|\n+-----------+-----+\n+-----------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1528116501117_1864075045","id":"20180602-191904_922011461","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:14-0300","dateFinished":"2018-06-04T11:19:15-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6968"},{"title":"Cria visão temporária para execução de consultas SQL","text":"%spark2\nlog_file_df_clean.createOrReplaceTempView(\"log_file_view\")","user":"admin","dateUpdated":"2018-06-04T11:19:18-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1528116501117_1864075045","id":"20180602-182223_473370248","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:18-0300","dateFinished":"2018-06-04T11:19:18-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6969"},{"title":"Pré-visualização dos dados no Matrix usando SQL","text":"%spark2.sql\nselect * from log_file_view LIMIT 5","user":"admin","dateUpdated":"2018-06-04T11:19:24-0300","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"host\ttimestamp\trequisicao\tcodigo_http\tbytes_retorno\nin24.inetnebr.com \t01/Aug/1995:00:00:01 -0400\t/shuttle/missions/sts-68/news/sts-68-mcc-05.txt\t200\t1839\nuplherc.upl.com \t01/Aug/1995:00:00:07 -0400\t/\t304\t0\nuplherc.upl.com \t01/Aug/1995:00:00:08 -0400\t/images/ksclogo-medium.gif\t304\t0\nuplherc.upl.com \t01/Aug/1995:00:00:08 -0400\t/images/MOSAIC-logosmall.gif\t304\t0\nuplherc.upl.com \t01/Aug/1995:00:00:08 -0400\t/images/USA-logosmall.gif\t304\t0\n"}]},"apps":[],"jobName":"paragraph_1528116501118_1865229292","id":"20180602-183901_1891150016","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:24-0300","dateFinished":"2018-06-04T11:19:24-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6970"},{"title":"Número de hosts únicos","text":"%spark2.sql\nSELECT COUNT(*) as total_distinct_hosts FROM (SELECT DISTINCT host FROM log_file_view)","user":"admin","dateUpdated":"2018-06-04T11:19:29-0300","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"total_distinct_hosts\n137979\n"}]},"apps":[],"jobName":"paragraph_1528116501118_1865229292","id":"20180602-183938_1357072516","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:29-0300","dateFinished":"2018-06-04T11:19:33-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6971"},{"title":"O total de erros 404","text":"%spark2.sql\nSELECT COUNT(*) as total_erro_404 FROM log_file_view where codigo_http = 404","user":"admin","dateUpdated":"2018-06-04T11:19:37-0300","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"total_erro_404\n20901\n"}]},"apps":[],"jobName":"paragraph_1528116501119_1864844543","id":"20180602-184703_35411611","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:37-0300","dateFinished":"2018-06-04T11:19:38-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6972"},{"title":"Os 5 URLs que mais causaram erro 404","text":"%spark2.sql\nSELECT requisicao, codigo_http, COUNT(*) as total_erro_404_by_url FROM log_file_view WHERE codigo_http = 404 GROUP BY requisicao, codigo_http ORDER BY COUNT(*) DESC LIMIT 5","user":"admin","dateUpdated":"2018-06-04T11:19:50-0300","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"requisicao","index":0,"aggr":"sum"}],"groups":[{"name":"codigo_http","index":1,"aggr":"sum"}],"values":[{"name":"total_erro_404_by_url","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"requisicao\tcodigo_http\ttotal_erro_404_by_url\n/pub/winvn/readme.txt\t404\t2004\n/pub/winvn/release.txt\t404\t1732\n/shuttle/missions/STS-69/mission-STS-69.html\t404\t682\n/shuttle/missions/sts-68/ksc-upclose.gif\t404\t426\n/history/apollo/a-001/a-001-patch-small.gif\t404\t384\n"}]},"apps":[],"jobName":"paragraph_1528116501119_1864844543","id":"20180602-190330_1486843050","dateCreated":"2018-06-04T09:48:21-0300","dateStarted":"2018-06-04T11:19:50-0300","dateFinished":"2018-06-04T11:19:52-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6973"},{"title":"Quantidade de erros 404 por dia","text":"%spark2.sql\nSELECT to_date(substr(timestamp, 1, 11),'dd/MMM/yyyy') as data, codigo_http, COUNT(*) as total_erro_404_by_dia from log_file_view WHERE codigo_http = 404 GROUP BY data, codigo_http ORDER BY data","user":"admin","dateUpdated":"2018-06-04T11:20:02-0300","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"data","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"total_erro_404_by_dia","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"data\tcodigo_http\ttotal_erro_404_by_dia\n1995-07-01\t404\t316\n1995-07-02\t404\t291\n1995-07-03\t404\t474\n1995-07-04\t404\t359\n1995-07-05\t404\t497\n1995-07-06\t404\t640\n1995-07-07\t404\t570\n1995-07-08\t404\t302\n1995-07-09\t404\t348\n1995-07-10\t404\t398\n1995-07-11\t404\t471\n1995-07-12\t404\t471\n1995-07-13\t404\t532\n1995-07-14\t404\t413\n1995-07-15\t404\t254\n1995-07-16\t404\t257\n1995-07-17\t404\t406\n1995-07-18\t404\t465\n1995-07-19\t404\t639\n1995-07-20\t404\t428\n1995-07-21\t404\t334\n1995-07-22\t404\t192\n1995-07-23\t404\t233\n1995-07-24\t404\t328\n1995-07-25\t404\t461\n1995-07-26\t404\t336\n1995-07-27\t404\t336\n1995-07-28\t404\t94\n1995-08-01\t404\t243\n1995-08-03\t404\t304\n1995-08-04\t404\t346\n1995-08-05\t404\t236\n1995-08-06\t404\t373\n1995-08-07\t404\t537\n1995-08-08\t404\t391\n1995-08-09\t404\t279\n1995-08-10\t404\t315\n1995-08-11\t404\t263\n1995-08-12\t404\t196\n1995-08-13\t404\t216\n1995-08-14\t404\t287\n1995-08-15\t404\t327\n1995-08-16\t404\t259\n1995-08-17\t404\t271\n1995-08-18\t404\t256\n1995-08-19\t404\t209\n1995-08-20\t404\t312\n1995-08-21\t404\t305\n1995-08-22\t404\t288\n1995-08-23\t404\t345\n1995-08-24\t404\t420\n1995-08-25\t404\t415\n1995-08-26\t404\t366\n1995-08-27\t404\t370\n1995-08-28\t404\t410\n1995-08-29\t404\t420\n1995-08-30\t404\t571\n1995-08-31\t404\t526\n"}]},"apps":[],"jobName":"paragraph_1528118209049_936849333","id":"20180604-101649_512351485","dateCreated":"2018-06-04T10:16:49-0300","dateStarted":"2018-06-04T11:20:02-0300","dateFinished":"2018-06-04T11:20:05-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6974"},{"title":"O total de bytes retornados","text":"%spark2.sql\nSELECT sum(bytes_retorno) as total_bytes_retorno from log_file_view","user":"admin","dateUpdated":"2018-06-04T11:20:19-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"total_bytes_retorno\n65524314915\n"}]},"apps":[],"jobName":"paragraph_1528118230672_162749870","id":"20180604-101710_788455585","dateCreated":"2018-06-04T10:17:10-0300","dateStarted":"2018-06-04T11:20:19-0300","dateFinished":"2018-06-04T11:20:20-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6975"},{"text":"%spark2.sql\n","user":"admin","dateUpdated":"2018-06-04T11:01:21-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528120663446_-1093655853","id":"20180604-105743_928263973","dateCreated":"2018-06-04T10:57:43-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6976"}],"name":"Testes Spark Lab Semantix","id":"2DESBF5RZ","angularObjects":{"2CHS8UYQQ:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}